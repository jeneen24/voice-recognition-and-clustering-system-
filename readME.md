# ğŸ™ï¸ Voice Clustering Project (TASK6)

This project implements a **multi-speaker voice clustering pipeline**.  
It processes raw audio recordings, extracts meaningful speech segments, generates speaker embeddings, and groups them into clusters (potential speakers).  
A simple **web interface** (Flask + HTML/CSS/JS) is provided to visualize the results and listen to the clustered audio samples.

---

## ğŸ“‚ Project Structure

TASK6/
â”œâ”€ data/
â”‚ â”œâ”€ raw/ # raw audio recordings (organized by speaker folders)
â”‚ â”œâ”€ clean/ # VAD-split speech segments
â”‚ â”œâ”€ embeddings/ # extracted speaker embeddings (.npy)
â”‚ â””â”€ results/ # clustering results (clusters.csv, plots, etc.)
â”œâ”€ src/
â”‚ â”œâ”€ preprocess.py # voice activity detection (VAD) and audio splitting
â”‚ â”œâ”€ vad.py # VAD helper functions (webrtcvad)
â”‚ â”œâ”€ embeddings.py # generate embeddings using Resemblyzer
â”‚ â”œâ”€ cluster.py # clustering (Agglomerative / KMeans)
â”‚ â”œâ”€ transcribe.py # (optional) speech-to-text using Whisper
â”‚ â””â”€ voice_clustering.py # full pipeline runner
â”œâ”€ web/
â”‚ â”œâ”€ static/
â”‚ â”‚ â”œâ”€ style.css
â”‚ â”‚ â”œâ”€ static.js
â”‚ â”‚ â””â”€ audio/ # copied audio files for playback in UI
â”‚ â””â”€ templates/
â”‚ â””â”€ index.html
â”œâ”€ app.py # Flask web server
â”œâ”€ requirements.txt
â””â”€ README.md


---

## ğŸš€ Features

1. **Preprocessing**
   - Splits raw audio into smaller speech segments using **WebRTC VAD**.
   - Ensures background noise and silence are removed.

2. **Speaker Embeddings**
   - Uses **Resemblyzer** to generate embeddings from each speech segment.
   - Stores embeddings as `.npy` files for clustering.

3. **Clustering**
   - Supports **Agglomerative Clustering** and **KMeans**.
   - Automatically estimates number of clusters if not provided.
   - Saves results into `data/results/clusters.csv`.

4. **Web Interface**
   - Displays all clusters with audio players for each segment.
   - Shows statistics (e.g., number of segments per cluster).
   - Interactive chart (using **Chart.js**) for cluster distribution.
   - "Refresh" button to reload updated clustering results.

5. **Extensibility**
   - Can integrate transcription (Whisper, SpeechBrain, PyAnnote).
   - Supports additional visualization (e.g., t-SNE/UMAP of embeddings).

---

## âš™ï¸ Installation

1. Clone this repository:
   


    Install Python dependencies:

pip install -r requirements.txt

Install system dependencies:

    sudo apt update
    sudo apt install ffmpeg sox

ğŸ¤ Usage
1. Prepare Data

Place your raw audio files inside:

data/raw/speaker1/
data/raw/speaker2/
...

2. Run Full Pipeline

python src/voice_clustering.py

This will:

    Split audio into clean speech segments

    Extract embeddings

    Perform clustering

    Save results into data/results/clusters.csv

3. Launch Web UI

python app.py

Open your browser at: http://127.0.0.1:5000
ğŸ“Š Web Interface

    Cluster Overview: Each cluster is displayed with:

        Cluster ID

        Number of audio segments

        List of audio samples with playback controls

    Statistics:

        Bar chart showing distribution of segments across clusters

    Actions:

        "Refresh Results" button reloads clustering output without restarting server

ğŸ§ª Example Workflow

    Raw audio:

data/raw/jackson/1.wav
data/raw/lucas/2.wav
data/raw/theo/3.wav

Run pipeline:

python src/voice_clustering.py

Generated outputs:

    Segments â†’ data/clean/jackson/jackson_seg0.wav ...

    Embeddings â†’ data/embeddings/jackson_seg0.npy ...

    Results â†’ data/results/clusters.csv

Web UI shows clusters, e.g.:

    Cluster 0 â†’ {jackson_seg0.wav, lucas_seg1.wav}
    Cluster 1 â†’ {theo_seg0.wav}



ğŸ› ï¸ Tech Stack

    Python: librosa, soundfile, webrtcvad, resemblyzer, scikit-learn, flask

    Frontend: HTML5, CSS3, JavaScript (Chart.js for charts)

    System: ffmpeg, sox